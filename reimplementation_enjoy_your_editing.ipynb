{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal implementation of Enjoy your Editing\n",
    "Zhuang, P., Koyejo, O. O., and Schwing, A. (2021). \n",
    "Enjoy your editing: Controllable GANs for image editing via latent space navigation. \n",
    "In International Conference on Learning Representations\n",
    "https://arxiv.org/pdf/2102.01187.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='./jupyter_imgs/enjoy_your_editing_architecture.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## index for target features\n",
    "'5_o_Clock_Shadow', 0\n",
    "'Arched_Eyebrows', 1\n",
    "'Attractive', 2\n",
    "'Bags_Under_Eyes', 3 \n",
    "'Bald', 4\n",
    "'Bangs', 5\n",
    "'Big_Lips', 6 \n",
    "'Big_Nose', 7\n",
    "'Black_Hair', 8 \n",
    "'Blond_Hair', 9\n",
    "'Blurry', 10\n",
    "'Brown_Hair', 11 \n",
    "'Bushy_Eyebrows', 12 \n",
    "'Chubby', 13\n",
    "'Double_Chin', 14 \n",
    "'Eyeglasses', 15\n",
    "'Goatee', 16\n",
    "'Gray_Hair', 17 \n",
    "'Heavy_Makeup', 18 \n",
    "'High_Cheekbones', 19 \n",
    "'Male', 20\n",
    "'Mouth_Slightly_Open', 21 \n",
    "'Mustache', 22\n",
    "'Narrow_Eyes', 23 \n",
    "'No_Beard', 24\n",
    "'Oval_Face', 25\n",
    "'Pale_Skin', 26\n",
    "'Pointy_Nose', 27\n",
    "'Receding_Hairline', 28 \n",
    "'Rosy_Cheeks', 29\n",
    "'Sideburns', 30\n",
    "'Smiling',31\n",
    "'Straight_Hair', 32 \n",
    "'Wavy_Hair', 33\n",
    "'Wearing_Earrings', 34 \n",
    "'Wearing_Hat', 35\n",
    "'Wearing_Lipstick', 36 \n",
    "'Wearing_Necklace', 37\n",
    "'Wearing_Necktie', 38\n",
    "'Young' 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stylegan2_path = \"./stylegan2-ada-pytorch\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(stylegan2_path)\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import click\n",
    "import dnnlib\n",
    "import legacy\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = \"cuda:0\"\n",
    "size = 1024\n",
    "truncation_psi = 0.5\n",
    "n_iters = 20001\n",
    "batch_size = 8\n",
    "random_start_seed = 0\n",
    "iter_start_seed = 0\n",
    "#d = torch.Tensor(np.random.RandomState(random_start_seed).normal(0, 0.002, [1, 512])).to(device)\n",
    "d = torch.Tensor(np.zeros([1, 512])).to(device)\n",
    "target_feature_index = 31\n",
    "lambda_regressor = 10.0\n",
    "lambda_content = 0.05\n",
    "lambda_gan = 0.05\n",
    "learning_rate = 0.0001\n",
    "\n",
    "noise_mode = 'const'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_folder = stylegan2_path + f'/training_runs/stylegan2/eye_start_zero_start_seed_{random_start_seed}_iter_start_seed_{iter_start_seed}_lambda_regressor_{lambda_regressor}_lambda_content_{lambda_content}_lambda_gan_{lambda_gan}_feature_{target_feature_index}_lr_{learning_rate}_batch_size{batch_size}'\n",
    "if not os.path.exists(logging_folder):\n",
    "    os.makedirs(logging_folder)\n",
    "\n",
    "if not os.path.exists(logging_folder + '/saved_latent_vecs'):\n",
    "    os.makedirs(logging_folder + '/saved_latent_vecs')\n",
    "\n",
    "with open(logging_folder + \"/training_log.txt\", \"a\") as myfile:\n",
    "            myfile.write(f'./training_runs/stylegan2/eye_start_zero_start_seed_{random_start_seed}_iter_start_seed_{iter_start_seed}_lambda_regressor_{lambda_regressor}_lambda_content_{lambda_content}_lambda_gan_{lambda_gan}_feature_{target_feature_index}_lr_{learning_rate}_batch_size{batch_size}' + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Normalization, self).__init__()\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "        self.mean = mean.clone().detach().view(-1, 1, 1)\n",
    "        self.std = std.clone().detach().view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bce_loss(pred, y, eps=1e-12):\n",
    "    loss = -(y * pred.clamp(min=eps).log() + (1 - y) * (1 - pred).clamp(min=eps).log()).mean()\n",
    "    return loss\n",
    "\n",
    "BCE_loss_logits = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_pkl = './pretrained_models/ffhq.pkl'\n",
    "\n",
    "with dnnlib.util.open_url(network_pkl) as f:\n",
    "    G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n",
    "\n",
    "with dnnlib.util.open_url(network_pkl) as f:\n",
    "    D = legacy.load_network_pkl(f)['D'].to(device)\n",
    "\n",
    "label = torch.zeros([1, G.c_dim], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "vgg19 = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "def get_content_loss(org_img, shifted_img):\n",
    "    content_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4']\n",
    "    norm = Normalization().to(device)\n",
    "    model = nn.Sequential(norm)\n",
    "\n",
    "    i = 0\n",
    "    content_losses = []\n",
    "    for layer in vgg19.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'bn_{}'.format(i)\n",
    "        else:\n",
    "            raise RuntimeError('Unrecognized layer: {}'\n",
    "                               .format(layer.__class__.__name__))\n",
    "        model.add_module(name, layer)\n",
    "        if name in content_layers:\n",
    "            org_content = model(org_img).detach()\n",
    "            shifted_content = model(shifted_img)\n",
    "            content_loss = F.mse_loss(org_content.detach(), shifted_content)\n",
    "            content_losses.append(content_loss)\n",
    "        \n",
    "    for i in range(len(content_losses)):\n",
    "        content_loss += content_losses[i]\n",
    "        content_loss = content_loss / len(content_losses)\n",
    "    return content_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def get_reg_module():\n",
    "    # Scene/Face, hard code resnet50 here\n",
    "    model = torch.hub.load('pytorch/vision:v0.5.0', 'resnet50', pretrained=False)\n",
    "    model.fc = torch.nn.Linear(2048, 40)\n",
    "    model = model.cuda()\n",
    "    ckpt = torch.load('./003_dict.model')\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    \"\"\"\n",
    "    If fine-tune or jointly train the classifier\n",
    "    \"\"\"\n",
    "    # optimizer.load_state_dict(ckpt['optm'])\n",
    "    # return model, optimizer\n",
    "    return model, None\n",
    "\n",
    "regressor, _ = get_reg_module()\n",
    "regressor.eval()\n",
    "'''\n",
    "regressor = torch.jit.load('./pretrained_models/resnet50.pth').eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='./jupyter_imgs/enjoy_your_editing_pseudocode.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.requires_grad = True\n",
    "optimizer = torch.optim.Adam([d], lr=learning_rate, betas=(0.5, 0.99))\n",
    "time1 = time.time()\n",
    "\n",
    "for i in range(n_iters):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Algorithm image step 2\n",
    "    z = torch.from_numpy(np.random.RandomState(i+iter_start_seed).randn(batch_size, 512)).to(device)\n",
    "    w = G.mapping(z,label, truncation_psi=truncation_psi) #  mapping z --> w, we use w instead of z as latent vec\n",
    "    epsilon = torch.from_numpy((np.random.RandomState(i+iter_start_seed).rand(batch_size) - 0.5)*2).to(device) # uniform distribution -1..1\n",
    "    \n",
    "    # Algorithm image step 3, computing the image and extract attributes with regressor\n",
    "    img_orig = G.synthesis(w, noise_mode=noise_mode)\n",
    "    \n",
    "    # scaling the img from 1024x1024 to 256x256 so it has the right size for the regressor\n",
    "    img_orig = F.interpolate(img_orig, size=256)\n",
    "    \n",
    "    alpha = regressor(img_orig)[:, target_feature_index]\n",
    "    \n",
    "    # Algorithm image step 4\n",
    "    delta = torch.clamp(alpha+epsilon, min=0.0, max=1.0) - alpha\n",
    "    \n",
    "    # Algorithm image step 5 and 6 compute shifted img\n",
    "    attribute_vector = (d*torch.transpose(torch.stack(512*[delta]), 0, 1))\n",
    "    attribute_vector_stacked18 = torch.stack([attribute_vector]*18).permute(1,0,2)\n",
    "    w_shifted = w + attribute_vector_stacked18\n",
    "    img_shifted = G.synthesis(w_shifted, noise_mode=noise_mode)\n",
    "    \n",
    "    # scaling the img from 1024x1024 to 256x256\n",
    "    img_shifted_256 = F.interpolate(img_shifted, size=256)\n",
    "    \n",
    "    alpha_prime = alpha + delta\n",
    "    \n",
    "    # Algorithm image step 7\n",
    "    alpha_shifted = regressor(img_shifted_256)[:, target_feature_index]\n",
    "    \n",
    "    # Algorithm step 8, calculate loss\n",
    "    content_loss = get_content_loss(img_orig, img_shifted_256)\n",
    "\n",
    "    regressor_loss = get_bce_loss(alpha_shifted, alpha_prime, eps=1e-12) # paper implementation\n",
    "    \n",
    "    discriminator_pred = D(img_shifted, c=label)\n",
    "    y_real = torch.autograd.Variable(torch.ones_like(discriminator_pred).to(device))\n",
    "    gan_loss = BCE_loss_logits(discriminator_pred, y_real)\n",
    "    \n",
    "    loss = lambda_regressor*regressor_loss + lambda_content*content_loss + lambda_gan*gan_loss\n",
    "    print('iter: ', i, ' loss:', loss.item())\n",
    "    \n",
    "    with open(logging_folder + \"/training_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('loss: ' + str(loss.item()) + '\\n')\n",
    "        \n",
    "    if i%100 == 0:\n",
    "        torch.save(d, logging_folder+'/saved_latent_vecs/latent_vec_' + str(i) + '.pt')\n",
    "    \n",
    "    # Algorithm step 9\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "time2 = time.time()\n",
    "print('training_time: ', time2 - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stylegan3",
   "language": "python",
   "name": "stylegan3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
