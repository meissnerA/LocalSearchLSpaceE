{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calc attribute and identity preservation\n",
    "### Mostly the implementation provided by https://github.com/KelestZ/Latent2im/blob/main/eval.py  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from natsort import natsorted\n",
    "import sys\n",
    "stylegan2_path = './stylegan2-ada-pytorch'\n",
    "sys.path.append(stylegan2_path)\n",
    "\n",
    "import dnnlib\n",
    "import click\n",
    "import legacy\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_regressor_predictions = 40 # the regressor is pretrained on CelebA and predicts 40 face attributes\n",
    "device = 'cuda:0'\n",
    "batch_size = 1\n",
    "truncation_psi = 0.5\n",
    "noise_mode = 'const'\n",
    "network_pkl =  './pretrained_models/ffhq.pkl'\n",
    "\n",
    "with dnnlib.util.open_url(network_pkl) as f:\n",
    "    G = legacy.load_network_pkl(f)['G_ema'].to(device) # stylegan2-generator\n",
    "\n",
    "with dnnlib.util.open_url(network_pkl) as f:\n",
    "    D = legacy.load_network_pkl(f)['D'].to(device)\n",
    "\n",
    "label = torch.zeros([1, G.c_dim], device=device)\n",
    "\n",
    "def initialize_model():\n",
    "    from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "    resnet = InceptionResnetV1(pretrained='vggface2').to(device).eval()\n",
    "    return resnet\n",
    "\n",
    "face_rec = initialize_model()\n",
    "regressor = torch.jit.load('./pretrained_models/resnet50.pth').eval().to(device)\n",
    "scaling_coeffs_eval = np.linspace(0, 1, 10) # 10 scaling factors for the edited images just like in Enjoy your Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## index for target features\n",
    "'5_o_Clock_Shadow', 0\n",
    "'Arched_Eyebrows', 1\n",
    "'Attractive', 2\n",
    "'Bags_Under_Eyes', 3 \n",
    "'Bald', 4\n",
    "'Bangs', 5\n",
    "'Big_Lips', 6 \n",
    "'Big_Nose', 7\n",
    "'Black_Hair', 8 \n",
    "'Blond_Hair', 9\n",
    "'Blurry', 10\n",
    "'Brown_Hair', 11 \n",
    "'Bushy_Eyebrows', 12 \n",
    "'Chubby', 13\n",
    "'Double_Chin', 14 \n",
    "'Eyeglasses', 15\n",
    "'Goatee', 16\n",
    "'Gray_Hair', 17 \n",
    "'Heavy_Makeup', 18 \n",
    "'High_Cheekbones', 19 \n",
    "'Male', 20\n",
    "'Mouth_Slightly_Open', 21 \n",
    "'Mustache', 22\n",
    "'Narrow_Eyes', 23 \n",
    "'No_Beard', 24\n",
    "'Oval_Face', 25\n",
    "'Pale_Skin', 26\n",
    "'Pointy_Nose', 27\n",
    "'Receding_Hairline', 28 \n",
    "'Rosy_Cheeks', 29\n",
    "'Sideburns', 30\n",
    "'Smiling',31\n",
    "'Straight_Hair', 32 \n",
    "'Wavy_Hair', 33\n",
    "'Wearing_Earrings', 34 \n",
    "'Wearing_Hat', 35\n",
    "'Wearing_Lipstick', 36 \n",
    "'Wearing_Necklace', 37\n",
    "'Wearing_Necktie', 38\n",
    "'Young' 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attribute_preservation_identity_preservation_buckets(attr_vec, target_attr_index):\n",
    "    '''\n",
    "    attr_vec: torch Tensor of shape [1, 512], which should change a semantically meaningful attribute \n",
    "              in the stylegan2-latent space\n",
    "    target_attr_index: int, which defines the attribute which should be changed, the index for each feature\n",
    "              is listed in the cell above\n",
    "    This function calculates the identity and attribute preservation for evaluating attribute vectors.\n",
    "    To have a the same amount of attribute change for a fair comparision of different approaches we \n",
    "    return interval_counter and normalize using the first element as described in our paper\n",
    "    '''\n",
    "    img_orig_segments = [[], [], []]\n",
    "    img_shifted_segments = [[], [], []]\n",
    "\n",
    "    pred_orig_segments = [[], [], []]\n",
    "    pred_shifted_segments = [[], [], []]\n",
    "    interval_counter = [0, 0, 0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for random_seed in range(1000):\n",
    "            z = torch.from_numpy(np.random.RandomState(random_seed).randn(batch_size, 512)).to(device)\n",
    "            w = G.mapping(z,label, truncation_psi=truncation_psi)\n",
    "            img_orig = G.synthesis(w, noise_mode=noise_mode)\n",
    "            img_orig = F.interpolate(img_orig, size=256) # reshape the image for the face_rec and regressor\n",
    "            pred_orig = regressor(img_orig).detach().cpu().numpy()\n",
    "\n",
    "            # as defined in the enjoy your editing paper we create 1000 original and 10,000 shifted images\n",
    "            # with an increasing amount of attribute change\n",
    "            for alpha in scaling_coeffs_eval: \n",
    "                delta = alpha - pred_orig[0, target_attr_index]\n",
    "                img_shifted = G.synthesis(w+attr_vec*delta, noise_mode=noise_mode)\n",
    "                img_shifted = F.interpolate(img_shifted, size=256)\n",
    "                pred_shifted = regressor(img_shifted).detach().cpu().numpy()\n",
    "                if np.abs(pred_shifted[0, target_attr_index] - pred_orig[0, target_attr_index]) <=0.3:\n",
    "                    img_orig_segments[0].append(img_orig)\n",
    "                    img_shifted_segments[0].append(img_shifted)\n",
    "                    pred_shifted_segments[0].append(pred_shifted)\n",
    "                    pred_orig_segments[0].append(pred_orig)\n",
    "                elif np.abs(pred_shifted[0, target_attr_index] - pred_orig[0, target_attr_index]) <=0.6:\n",
    "                    img_orig_segments[1].append(img_orig)\n",
    "                    img_shifted_segments[1].append(img_shifted)\n",
    "                    pred_shifted_segments[1].append(pred_shifted)\n",
    "                    pred_orig_segments[1].append(pred_orig)\n",
    "                elif np.abs(pred_shifted[0, target_attr_index] - pred_orig[0, target_attr_index]) <=0.9:\n",
    "                    img_orig_segments[2].append(img_orig)\n",
    "                    img_shifted_segments[2].append(img_shifted)\n",
    "                    pred_shifted_segments[2].append(pred_shifted)\n",
    "                    pred_orig_segments[2].append(pred_orig)\n",
    "\n",
    "        embeddings = []\n",
    "        sim = [[], [], []]\n",
    "        interval_counter[0] += len(pred_orig_segments[0])\n",
    "        interval_counter[1] += len(pred_orig_segments[1])\n",
    "        interval_counter[2] += len(pred_orig_segments[2])\n",
    "\n",
    "        for k in range(3): # we have the 3 segments (0-0.3, 0.3-0.6, 0.6-0.9) defined above\n",
    "            for i in range(len(img_shifted_segments[k])):\n",
    "                # Compute the embedding of the original image\n",
    "                img_org_np = np.uint8(np.clip(((img_orig_segments[k][i].cpu().numpy() + 1) / 2.0) * 255, 0, 255))\n",
    "                org = Image.fromarray(np.transpose(img_org_np[0], (1,2,0)))\n",
    "                reshaped_org = org.resize((160, 160))\n",
    "                reshaped_org = torch.Tensor(np.transpose(np.array(reshaped_org), (2,0,1))).to(device).unsqueeze(0)\n",
    "                embed_org = face_rec(reshaped_org)\n",
    "\n",
    "                # Compute the embedding of the edited image\n",
    "                img_shifted_np = np.uint8(np.clip(((img_shifted_segments[k][i].cpu().numpy() + 1) / 2.0) * 255, 0, 255))\n",
    "                img_shifted = Image.fromarray(np.transpose(img_shifted_np[0], (1, 2, 0)))\n",
    "                reshaped_shifted_img = img_shifted.resize((160, 160))\n",
    "                reshaped_shifted_img = torch.Tensor(np.transpose(np.array(reshaped_shifted_img), (2, 0, 1))).to(device).unsqueeze(0)\n",
    "\n",
    "                embed = face_rec(reshaped_shifted_img)\n",
    "                # Compute the Cosine similarity for image identity preservation\n",
    "                similarity = cosine(embed.detach().cpu().numpy(), embed_org.detach().cpu().numpy())\n",
    "                sim[k].append(similarity)\n",
    "\n",
    "        results_avg = []\n",
    "        results_std = []\n",
    "\n",
    "        for k in range(3):\n",
    "            if len(sim[k]) == 0:\n",
    "                continue\n",
    "            result_avg = 1-np.mean(sim[k])\n",
    "            result_std = np.array(sim[k]).std()\n",
    "            results_avg.append(result_avg)\n",
    "            results_std.append(result_std)\n",
    "\n",
    "        print('[IDENTITY PRESERVATION MEAN] Results on 3 epsilon segments', ['%.4f' % i for i in results_avg])\n",
    "        print('[IDENTITY PRESERVATION STD ] Results on 3 epsilon segments', ['%.4f' % i for i in results_std])\n",
    "\n",
    "        multi_attrs = [[], [], []]\n",
    "        original_attrs = [[], [], []]\n",
    "\n",
    "        multi_attrs[0] += pred_shifted_segments[0]\n",
    "        multi_attrs[1] += pred_shifted_segments[1]\n",
    "        multi_attrs[2] += pred_shifted_segments[2]\n",
    "\n",
    "        original_attrs[0] += pred_orig_segments[0]\n",
    "        original_attrs[1] += pred_orig_segments[1]\n",
    "        original_attrs[2] += pred_orig_segments[2]\n",
    "\n",
    "        for k in range(3):\n",
    "            multi_attrs[k] = np.array(multi_attrs[k])\n",
    "            original_attrs[k] = np.array(original_attrs[k])\n",
    "\n",
    "        results_avg = []\n",
    "        results_std = []\n",
    "\n",
    "        for k in range(3):\n",
    "            if (original_attrs[k].shape[0] == 0):\n",
    "                continue\n",
    "\n",
    "            org = np.hstack([original_attrs[k][:, :int(target_attr_index)], original_attrs[k][:, int(target_attr_index + 1):]])\n",
    "            changed = np.hstack([multi_attrs[k][:, :int(target_attr_index)], multi_attrs[k][:, int(target_attr_index + 1):]])\n",
    "            \n",
    "            result_avg = np.mean(np.abs(changed - org))\n",
    "            result_std = np.abs(changed - org).std()\n",
    "            \n",
    "            results_avg.append(result_avg)\n",
    "            results_std.append(result_std)\n",
    "\n",
    "        print('[ATTRIBUTE PRESERVATION MEAN] Results on 3 epsilon segments', ['%.4f' % i for i in results_avg])\n",
    "        print('[ATTRIBUTE PRESERVATION STD ] Results on 3 epsilon segments', ['%.4f' % i for i in results_std])\n",
    "        \n",
    "        print('intervall_counter ', interval_counter)\n",
    "        print('===========================================')\n",
    "        print('attribute vector length', attr_vec.norm())\n",
    "        return interval_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr_vector_scaling(attr_vec, reference_bucket, stepsize, optimization_direction, scaling_coeff):\n",
    "    '''\n",
    "    attr_vec: torch Tensor of shape [1, 512], which should change a semantically meaningful attribute \n",
    "              in the stylegan2-latent space\n",
    "    reference_bucket: int, input first element of a reference interval_counter to find a scaling for an attribute \n",
    "              vector such that the first element of interval_counter is reference_bucket +/- 1%\n",
    "    stepsize: int, starting step size for optimization\n",
    "    optimization_direction: int, defines if we start to increase or decrease the scaling_coeff\n",
    "    scaling_coeff: int, initial scaling coefficient to get the same interval_counter[0] as reference_bucket\n",
    "              gets optimized with this function\n",
    "    This function is a quick implementation to find a scaling_coeff for attr_vec, such that the \n",
    "    interval_counter[0] == reference_bucket +/- 1%\n",
    "    \n",
    "    '''\n",
    "    for i in range(20):\n",
    "        intervall_counter = get_attribute_preservation_identity_preservation_buckets(attr_vec*scaling_coeff, target_attr_index)\n",
    "        first_bucket = intervall_counter[0]\n",
    "        if np.abs(first_bucket - reference_bucket) < reference_bucket*0.01:\n",
    "            break\n",
    "        elif first_bucket - reference_bucket < 0:\n",
    "            scaling_coeff -= stepsize\n",
    "            if optimization_direction == -1:\n",
    "                stepsize /= 2\n",
    "            optimization_direction = 1\n",
    "        else:\n",
    "            scaling_coeff += stepsize\n",
    "            if optimization_direction == 1:\n",
    "                stepsize /= 2\n",
    "            optimization_direction = -1\n",
    "        print(stepsize, intervall_counter, reference_bucket, scaling_coeff)\n",
    "    print(stepsize, intervall_counter, reference_bucket, scaling_coeff)\n",
    "    return scaling_coeff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smiling\n",
    "attr_vec_zhuang_smile = torch.load(\"./stylegan2-ada-pytorch/training_runs/stylegan2/eye_start_zero_start_seed_0_iter_start_seed_0_lambda_regressor_10.0_lambda_content_0.05_lambda_gan_0.05_feature_31_lr_0.0001_batch_size1/saved_latent_vecs/latent_vec_20000.pt\", map_location=device)\n",
    "d_np = np.load('./stylegan2-ada-pytorch/training_runs/stylegan2/shen/shen_smiling_w.npy')\n",
    "attr_vec_shen_smile = torch.Tensor([d_np]).to(device)\n",
    "attr_vec_ours_bs1_smile = torch.load(\"./stylegan2-ada-pytorch/training_runs/stylegan2/our_approach_feature_31_maxLenght_0.8_lr_0.0003_batch_size1/saved_latent_vecs/latent_vec_61000.pt\", map_location=device)\n",
    "attr_vec_ours_bs8_smile = torch.load(\"./stylegan2-ada-pytorch/training_runs/stylegan2/our_approach_feature_31_maxLenght_0.8_lr_0.0003_batch_size8/saved_latent_vecs/latent_vec_12000.pt\", map_location=device)\n",
    "###########################################\n",
    "# hair color\n",
    "attr_vec_zhuang_hair = torch.load(\"./stylegan2-ada-pytorch/training_runs/stylegan2/eye_start_zero_start_seed_0_iter_start_seed_20000_lambda_regressor_10.0_lambda_content_0.05_lambda_gan_0.05_feature_9_lr_0.0001_batch_size1/saved_latent_vecs/latent_vec_20000.pt\", map_location=device)\n",
    "d_np = np.load('./stylegan2-ada-pytorch/training_runs/stylegan2/shen/shen_hair_col_9_w.npy')\n",
    "attr_vec_shen_hair = torch.Tensor([d_np]).to(device)\n",
    "attr_vec_ours_bs1_hair = torch.load(\"./stylegan2-ada-pytorch/training_runs/stylegan2/our_approach_feature_9_maxLenght_0.8_lr_0.0003_batch_size1_seed_offset_0/saved_latent_vecs/latent_vec_61000.pt\", map_location=device)\n",
    "attr_vec_ours_bs8_hair = torch.load(\"./stylegan2-ada-pytorch/training_runs/stylegan2/our_approach_feature_9_maxLenght_0.8_lr_0.0003_batch_size8_seed_offset_0/saved_latent_vecs/latent_vec_12000.pt\", map_location=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_attr_index = 31 # smiling\n",
    "#target_attr_index = 9 # hair_col\n",
    "\n",
    "print(\"Get the first Bucket of Zhuang for the attribute Smiling as a reference to scale the other vectors\")\n",
    "buckets_zhuang = get_attribute_preservation_identity_preservation_buckets(attr_vec_zhuang_smile, target_attr_index)\n",
    "reference_bucket = buckets_zhuang[0]\n",
    "stepsize = 0.5\n",
    "optimization_direction = 1\n",
    "scaling_coeff = 4.0\n",
    "\n",
    "print(\"Find the right scaling such that the first bucket has as many samples as Zhuang +/- 1%\")\n",
    "get_attr_vector_scaling(attr_vec_shen_smile, reference_bucket, stepsize, optimization_direction, scaling_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smiling\n",
    "# scaling zhuang smiling = 1.0\n",
    "print(\"Zhuang smiling\")\n",
    "get_attribute_preservation_identity_preservation_buckets(attr_vec_zhuang_smile*1.0, 31)\n",
    "print(\"----------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "# scaling shen smiling = 1.31\n",
    "print(\"Shen smiling\")\n",
    "get_attribute_preservation_identity_preservation_buckets(attr_vec_shen_smile*1.31, 31)\n",
    "print(\"----------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "# scaling ours_bs1 smiling = 1.75\n",
    "print(\"Ours Batch Size=1 smiling\")\n",
    "get_attribute_preservation_identity_preservation_buckets(attr_vec_ours_bs1_smile*1.75, 31)\n",
    "print(\"----------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "# scaling ours_bs8 smiling = 1.625\n",
    "print(\"Ours Batch Size=8 smiling\")\n",
    "get_attribute_preservation_identity_preservation_buckets(attr_vec_ours_bs8_smile*1.625, 31)\n",
    "print(\"----------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "###########################################\n",
    "# hair color\n",
    "\n",
    "# scaling zhuang hair = 1.0\n",
    "print(\"Zhuang Hair9\")\n",
    "get_attribute_preservation_identity_preservation_buckets(attr_vec_zhuang_hair*1.0, 9)\n",
    "print(\"----------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "# scaling shen hair = 2.4375\n",
    "print(\"Shen Hair9\")\n",
    "get_attribute_preservation_identity_preservation_buckets(attr_vec_shen_hair*2.4375, 9)\n",
    "print(\"----------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "# scaling ours_bs1 = 4\n",
    "print(\"Ours Hair9\")\n",
    "get_attribute_preservation_identity_preservation_buckets(attr_vec_ours_bs1_hair*4.0, 9)\n",
    "print(\"----------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "# scaling ours_bs8 = 3.625\n",
    "print(\"Ours Hair9\")\n",
    "get_attribute_preservation_identity_preservation_buckets(attr_vec_ours_bs8_hair*3.625, 9)\n",
    "print(\"----------------------------------------\")\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize influece of attribute vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tensor(tensor):\n",
    "    tensor = tensor.clone()  # avoid modifying tensor in-place\n",
    "\n",
    "    def norm_ip(img, min, max):\n",
    "        img.clamp_(min=min, max=max)\n",
    "        img.add_(-min).div_(max - min + 1e-5)\n",
    "        return img\n",
    "\n",
    "    return norm_ip(tensor, float(tensor.min()), float(tensor.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "random_seed = 0\n",
    "truncation_psi = 0.5\n",
    "noise_mode = 'const'\n",
    "label = torch.zeros([1, G.c_dim], device=device)\n",
    "z = torch.from_numpy(np.random.RandomState(random_seed).randn(1, G.z_dim)).to(device)\n",
    "w = G.mapping(z,label, truncation_psi=truncation_psi)\n",
    "img_pt1 = G.synthesis(w, noise_mode=noise_mode)\n",
    "\n",
    "img = normalize_tensor(img_pt1[0])\n",
    "img = img.cpu().numpy()\n",
    "img = np.rollaxis(img, 0, 3)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pt1 = G.synthesis(w+attr_vec_ours_bs8_hair*3.625, noise_mode=noise_mode)\n",
    "\n",
    "img = normalize_tensor(img_pt1[0])\n",
    "img = img.detach().cpu().numpy()\n",
    "img = np.rollaxis(img, 0, 3)\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stylegan3",
   "language": "python",
   "name": "stylegan3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
